{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9249cb9-b649-4b02-8ce0-b901e8f3afad",
   "metadata": {},
   "source": [
    "# Test Parallel DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b713f6b-c3a2-4ec1-b77c-e690df7ed718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import threading\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import gym\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5258a7-a507-4e06-8275-712f580cb669",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_episode = 1000\n",
    "discount_factor = 0.99\n",
    "learning_rate = 2e-4\n",
    "test_frequency = 10\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.02\n",
    "batch_size = 64\n",
    "size_replay_buffer = 10000\n",
    "update_frequency = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aabb6bf-c7cc-4865-9302-c0b8174828de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestAgent(threading.Thread) :\n",
    "\n",
    "    speed_model = 1 # Vitesse\n",
    "    condition = True\n",
    "\n",
    "    def __init__(self,\n",
    "                 nb_test : int,\n",
    "                 q_network\n",
    "                ):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.nb_test = nb_test\n",
    "        self.q_network = q_network\n",
    "        self.list_rewards_mean = list()\n",
    "        self.list_rewards_std = list()\n",
    "\n",
    "    def run(self) :\n",
    "        while(self.condition) :\n",
    "            time.sleep(TestAgent.speed_model)\n",
    "            list_rewards = list()\n",
    "            for i in range(self.nb_test) :\n",
    "                list_rewards.append(self.test())\n",
    "            list_rewards = np.array(list_rewards)\n",
    "            self.list_rewards_mean.append( list_rewards.mean() )\n",
    "            self.list_rewards_std.append( list_rewards.std() )\n",
    "            print(f\"mean : {list_rewards.mean()}\")\n",
    "            print(f\"std : {list_rewards.std()}\")\n",
    "            \n",
    "            \n",
    "    def test(self) : \n",
    "        timestepmax = 2000\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        cum_sum = 0\n",
    "        iteration = 0\n",
    "        while not done and iteration < timestepmax:\n",
    "            state_t = torch.as_tensor(state , dtype = torch.float32)\n",
    "            action = torch.argmax(self.q_network(state_t)).item()\n",
    "            new_state,reward,done,_ = env.step(action)\n",
    "            state = new_state\n",
    "            cum_sum += reward\n",
    "            iteration += 1\n",
    "        return cum_sum\n",
    "                    \n",
    "    def setCondition(self,condition : bool):\n",
    "        self.condition = condition\n",
    "        print(\"Arret de l'agent de test\")\n",
    "\n",
    "class CollectorAgent(threading.Thread) :\n",
    "\n",
    "    speed_model = 0 # Vitesse\n",
    "    condition = True\n",
    "\n",
    "    def __init__(self,\n",
    "                replay_buffer : list,\n",
    "                q_network\n",
    "                ):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.q_network = q_network\n",
    "        \n",
    "    def run(self) :\n",
    "        while(self.condition) :\n",
    "            time.sleep(CollectorAgent.speed_model)\n",
    "            self.collect_trajectory()\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "    def collect_trajectory(self) :\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done :\n",
    "            state_t = torch.as_tensor(state , dtype = torch.float32)\n",
    "            if random.random() > epsilon :\n",
    "                action = torch.argmax(q_network(state_t)).item()\n",
    "            else :\n",
    "                action = env.action_space.sample()\n",
    "                \n",
    "            new_state,reward,done,_ = env.step(action)\n",
    "            transition = (state,action,done,reward,new_state)\n",
    "            replay_buffer.append(transition)\n",
    "            state = new_state\n",
    "            \n",
    "            \n",
    "    def setCondition(self,condition : bool):\n",
    "        self.condition = condition\n",
    "        print(\"Arret de l'agent de collecte\")\n",
    "        \n",
    "        \n",
    "        \n",
    "class LearnerAgent(threading.Thread) :\n",
    "\n",
    "    speed_model = 0 # Vitesse\n",
    "    condition = True\n",
    "\n",
    "    def __init__(self,\n",
    "                q_target_network,\n",
    "                q_network,\n",
    "                replay_buffer,\n",
    "                optimizer\n",
    "                ):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.q_network = q_network\n",
    "        self.q_target_network = q_target_network\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    def run(self) :\n",
    "        while(self.condition) :\n",
    "            time.sleep(CollectorAgent.speed_model)\n",
    "            self.collect_trajectory() \n",
    "            \n",
    "            \n",
    "            \n",
    "    def learn(self) :\n",
    "        \n",
    "        if len(self.replay_buffer) >= batch_size :\n",
    "            \n",
    "            batch = random.sample(self.replay_buffer,batch_size)\n",
    "\n",
    "            states = np.asarray([exp[0] for exp in batch],dtype=np.float32)\n",
    "            actions = np.asarray([exp[1] for exp in batch],dtype=int)\n",
    "            dones = np.asarray([exp[2] for exp in batch],dtype=int)\n",
    "            rewards = np.asarray([exp[3] for exp in batch],dtype=np.float32)\n",
    "            new_states = np.asarray([exp[4] for exp in batch],dtype=np.float32)\n",
    "            \n",
    "            states_t = torch.as_tensor(states , dtype=torch.float32)\n",
    "            dones_t = torch.as_tensor(dones , dtype = torch.int64)\n",
    "            new_states_t = torch.as_tensor(new_states , dtype=torch.float32)\n",
    "            actions_t = torch.as_tensor(actions , dtype = torch.int64).unsqueeze(1)\n",
    "            rewards_t = torch.as_tensor(rewards , dtype=torch.float32)\n",
    "            \n",
    "            y_target = rewards_t + discount_factor * (1 - dones_t) * torch.max(self.q_target_network(new_states_t),dim=1)[0].detach()\n",
    "\n",
    "            mse = nn.MSELoss()\n",
    "\n",
    "            loss = mse(torch.gather(self.q_network(states_t),dim=1,index=actions_t), y_target.unsqueeze(1))\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            for target_param, local_param in zip(self.q_target_network.parameters(), self.q_network.parameters()):\n",
    "                target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)  \n",
    "                \n",
    "                \n",
    "    def setCondition(self,condition : bool):\n",
    "        self.condition = condition\n",
    "        print(\"Arret de l'agent de learn\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03a1f8a6-ba99-41f2-9a13-444d8f85ca16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[1, 1]\n",
      "[1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "l = list()\n",
    "\n",
    "test_agent = TestAgent(l)\n",
    "collector_agent = CollectorAgent(l)\n",
    "learner_agent = LearnerAgent()\n",
    "\n",
    "test_agent.start()\n",
    "collector_agent.start()\n",
    "\n",
    "time.sleep(20)\n",
    "\n",
    "test_agent.setCondition(False)\n",
    "collector_agent.setCondition(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5a3f59-6638-4a59-80d1-17a028e9d269",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module) :\n",
    "    \n",
    "    def __init__(self,\n",
    "              nb_actions,\n",
    "              nb_observations) : \n",
    "        \n",
    "        super().__init__()\n",
    "        self.nb_actions = nb_actions\n",
    "        self.nb_observations = nb_observations\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(nb_observations, 125),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(125,100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, nb_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x) :\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d3a0ee-2287-408e-a8a1-9b58015ec672",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "nb_episode = 1000\n",
    "\n",
    "discount_factor = 0.99\n",
    "learning_rate = 2e-4\n",
    "test_frequency = 10\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.02\n",
    "batch_size = 64\n",
    "size_replay_buffer = int(1e5)\n",
    "update_frequency = 1\n",
    "\n",
    "tau = 1e-3 \n",
    "\n",
    "\n",
    "\n",
    "replay_buffer = deque(maxlen=size_replay_buffer)\n",
    "q_network = QNetwork(nb_actions,nb_observations)\n",
    "q_target_network = QNetwork(nb_actions,nb_observations)\n",
    "q_target_network.load_state_dict(q_network.state_dict())\n",
    "optimizer = torch.optim.Adam(q_network.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811e3ca6-30f5-4f27-9e81-b9a4741f0c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = deque(maxlen=size_replay_buffer)\n",
    "q_network = QNetwork(nb_actions,nb_observations)\n",
    "\n",
    "timestep = 0\n",
    "\n",
    "optimizer = torch.optim.Adam(q_network.parameters(), lr=learning_rate)\n",
    "list_tests_2 = []\n",
    "\n",
    "average_list = deque(maxlen=100)\n",
    "for episode in tqdm(range(nb_episode)) :\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    cumul = 0\n",
    "    epsilon = max(epsilon * epsilon_decay,epsilon_min)\n",
    "    \n",
    "    while not done : \n",
    "        state_t = torch.as_tensor(state , dtype = torch.float32)\n",
    "        \n",
    "        if random.random() > epsilon :\n",
    "            action = torch.argmax(q_network(state_t)).item()\n",
    "        else :\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        new_state,reward,done,_ = env.step(action)\n",
    "        \n",
    "        cumul += reward\n",
    "        \n",
    "        transition = (state,action,done,reward,new_state)\n",
    "        replay_buffer.append(transition)\n",
    "        \n",
    "        if len(replay_buffer) >= batch_size and timestep % update_frequency == 0 :\n",
    "        \n",
    "            \n",
    "            batch = random.sample(replay_buffer,batch_size)\n",
    "\n",
    "            states = np.asarray([exp[0] for exp in batch],dtype=np.float32)\n",
    "            actions = np.asarray([exp[1] for exp in batch],dtype=int)\n",
    "            dones = np.asarray([exp[2] for exp in batch],dtype=int)\n",
    "            rewards = np.asarray([exp[3] for exp in batch],dtype=np.float32)\n",
    "            new_states = np.asarray([exp[4] for exp in batch],dtype=np.float32)\n",
    "\n",
    "            \n",
    "            states_t = torch.as_tensor(states , dtype=torch.float32)\n",
    "            dones_t = torch.as_tensor(dones , dtype = torch.int64)\n",
    "            new_states_t = torch.as_tensor(new_states , dtype=torch.float32)\n",
    "            actions_t = torch.as_tensor(actions , dtype = torch.int64).unsqueeze(1)\n",
    "            rewards_t = torch.as_tensor(rewards , dtype=torch.float32)\n",
    "            \n",
    "            y_target = rewards_t + discount_factor * (1 - dones_t) * torch.max(q_network(new_states_t),dim=1)[0]\n",
    "\n",
    "            mse = nn.MSELoss()\n",
    "\n",
    "            loss = mse(torch.gather(q_network(states_t),dim=1,index=actions_t), y_target.unsqueeze(1))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        state = new_state\n",
    "        timestep += 1\n",
    "    \n",
    "    average_list.append(cumul)\n",
    "    if episode % test_frequency == 0 :\n",
    "        t =  0\n",
    "        for _ in range(10) :\n",
    "            t += test(q_network)\n",
    "        t /= 10\n",
    "        avg = sum(average_list) / len(average_list)\n",
    "        print(f\"episode {episode} - test reward : {t} - avg : {avg} - epsilon {epsilon}\")\n",
    "        list_tests_2.append(t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
