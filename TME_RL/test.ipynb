{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b52cb2b-bd84-4d9d-8aff-bbd9a0f358a9",
   "metadata": {},
   "source": [
    "# Test pour discretiser l'espace d'état du jeu Flappy Bird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aac90ef9-ab84-4b15-bf9c-80e01b0e6be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2615e6e9-5873-4112-9277-e6706c094ed2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35243/22658070.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmax_distance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_distance\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_distance\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# [float,float]\n",
    "\n",
    "def arg_state(value,tab):\n",
    "    bestIndex = 0\n",
    "    index = 0\n",
    "    while index < len(tab) and value > tab[index]:\n",
    "        bestIndex = index\n",
    "        index+=1\n",
    "    return bestIndex\n",
    "    \n",
    "    \n",
    "step = 30\n",
    "min_distance = 0\n",
    "max_distance = 2\n",
    "\n",
    "tab = np.linspace(min_distance,max_distance,step)\n",
    "print(tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa050fdc-2f4d-4a91-a841-e982376f6f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.9904\n",
      "(300, -10, 8)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "xdif = 308\n",
    "\"\"\"\n",
    "if xdif < 300:\n",
    "    xdif = int(xdif) - (int(xdif) % 5)\n",
    "else:\n",
    "    xdif = 300\n",
    "print(xdif)\n",
    "\"\"\"\n",
    "\n",
    "ydif = -1.9904\n",
    "\"\"\"\n",
    "if ydif > 10 : \n",
    "    ydif = 10\n",
    "elif ydif > 0 : \n",
    "    ydif = int(ydif) - (int(ydif) % 1)\n",
    "elif ydif < -10 : \n",
    "    ydif = -10\n",
    "elif ydif < 0 :\n",
    "    ydif = int(ydif) + (int(ydif) % 1)\n",
    "\"\"\"\n",
    "print(ydif)\n",
    "\n",
    "def get_state2(xdif,ydif,speed):\n",
    "    hauteur_carte = 512\n",
    "    largeur_carte = 288\n",
    "    xdif *= largeur_carte\n",
    "    ydif *= hauteur_carte\n",
    "    if xdif < 300:\n",
    "        xdif = int(xdif) - (int(xdif) % 5)\n",
    "    else:\n",
    "        xdif = 300\n",
    "    if ydif > 10 : \n",
    "        ydif = 10\n",
    "    elif ydif > 0 : \n",
    "        ydif = int(ydif) - (int(ydif) % 1)\n",
    "    elif ydif < -10 : \n",
    "        ydif = -10\n",
    "    elif ydif < 0 :\n",
    "        ydif = int(ydif) + (int(ydif) % 1)\n",
    "    return (xdif,ydif,speed)\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "states = [(j,i,z) for i in range(-10,11,1) for j in range(0,305,5) for z in range(-10,11)]\n",
    "Q = dict()\n",
    "actions = [0,1]\n",
    "for state in states:\n",
    "    for action in actions :\n",
    "        Q[(state,action)] = 0\n",
    "        \n",
    "        \n",
    "#print(Q)  \n",
    "print(get_state2(xdif,ydif,8))\n",
    "print(Q[(get_state2(xdif,ydif,8),1)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21f0b31d-765c-4025-8e07-1e2a16d5d801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "475.2 -5.9904\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hauteur_carte = 512\n",
    "largeur_carte = 288\n",
    "\n",
    "wvalue = 1.65 * largeur_carte\n",
    "hvalue = -0.0117 * hauteur_carte\n",
    "\n",
    "print(wvalue , hvalue)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59765d7f-fc4a-4760-b26d-89dbb6673685",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35243/3039515489.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tab' is not defined"
     ]
    }
   ],
   "source": [
    "value = 50\n",
    "print(arg_state(value,tab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7abce0b7-57a4-4455-bb1a-06e54276dbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import flappy_bird_gym\n",
    "import time\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class MyFlappyEnv:\n",
    "    \"\"\" Custom Flappy Env :\n",
    "        * state : [horizontal delta of the next pipe, vertical delta, vertical velocity]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.env = flappy_bird_gym.make('FlappyBird-v0')\n",
    "        self.env._normalize_obs = False\n",
    "        self._last_score = 0\n",
    "    def __getattr__(self,attr):\n",
    "        return self.env.__getattribute__(attr)\n",
    "    \n",
    "    def step(self,action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        if done:\n",
    "            reward -=1000\n",
    "        player_x = self.env._game.player_x\n",
    "        player_y = self.env._game.player_y\n",
    "\n",
    "        return np.hstack([obs,self.env._game.player_vel_y]),reward, done, info\n",
    "    def reset(self):\n",
    "        return np.hstack([self.env.reset(),self.env._game.player_vel_y])\n",
    "\n",
    "def test_gym(fps=30):\n",
    "    env = gym.make('Taxi-v3')\n",
    "    env.reset()\n",
    "    r = 0\n",
    "    for i in range(100):\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        r += reward\n",
    "        env.render()\n",
    "        time.sleep(1/fps)\n",
    "        print(f\"iter {i} : action {action}, reward {reward}, state {type(obs)} \")\n",
    "        if done:\n",
    "            break\n",
    "    print(f\"reward cumulatif : {r} \")\n",
    " \n",
    "def test_flappy(fps=30):\n",
    "    env = flappy_bird_gym.make('FlappyBird-v0')\n",
    "    env.reset()\n",
    "    r = 0\n",
    "    for i in range(100):\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        r += reward\n",
    "        env.render()\n",
    "        time.sleep(1/fps)\n",
    "        print(f\"iter {i} : action {action}, reward {reward}, state {obs} {info}, {env._game.player_vel_y}\")\n",
    "        print(\"data : \",env._game)\n",
    "        if done:\n",
    "            break\n",
    "    print(f\"reward cumulatif : {r} \")\n",
    " \n",
    "def play_env(agent,max_ep=500,fps=-1,verbose=False):\n",
    "    \"\"\"\n",
    "        Play an episode :\n",
    "        * agent : agent with two functions : act(state) -> action, and store(state,action,state,reward)\n",
    "        * max_ep : maximal length of the episode\n",
    "        * fps : frame per second,not rendering if <=0\n",
    "        * verbose : True/False print debug messages\n",
    "        * return the cumulative reward\n",
    "    \"\"\"\n",
    "    obs = agent.env.reset()\n",
    "    cumr = 0\n",
    "    for i in range(max_ep):\n",
    "        last_obs = obs\n",
    "        action = agent.act(obs)\n",
    "        obs,reward,done,info = agent.env.step(int(action))\n",
    "        agent.store(last_obs,action,obs,reward)\n",
    "        cumr += reward\n",
    "        if fps>0:\n",
    "            agent.env.render()\n",
    "            if verbose: print(f\"iter {i} : {action}: {reward} -> {obs} \")        \n",
    "            time.sleep(1/fps)\n",
    "        if done:\n",
    "            break\n",
    "    return cumr\n",
    "\n",
    "def transform_policy(env,policy) :\n",
    "    # On transforme notre dictionnaire en Pi : State -> Int (indice action)\n",
    "    new_policy = dict()\n",
    "    for state in policy:\n",
    "        actions = env.env.P[state]\n",
    "        for action in actions :\n",
    "            if actions[action] == policy[state]:\n",
    "                new_policy[state] = action\n",
    "    return new_policy\n",
    "\n",
    "class AgentRandom:\n",
    "    \"\"\"\n",
    "         A simple random agent\n",
    "    \"\"\"\n",
    "    def __init__(self,env):\n",
    "        self.env = env\n",
    "\n",
    "    def act(self,obs):\n",
    "        return self.env.action_space.sample()\n",
    "\n",
    "    def store(self,obs,action,new_obs,reward):\n",
    "        pass\n",
    "\n",
    "class AgentPolicy:\n",
    "    \"\"\"\n",
    "        Agent following a policy pi : pi is a dictionary state -> action\n",
    "    \"\"\"\n",
    "    def __init__(self,env,pi):\n",
    "        self.env = env\n",
    "        self.pi = pi\n",
    "\n",
    "    def act(self,obs):\n",
    "        return self.pi[obs]\n",
    "\n",
    "    def store(self,obs,action,new_obs,reward):\n",
    "        pass\n",
    "\n",
    "    def getPi(self) :\n",
    "        return self.pi\n",
    "\n",
    "    def getEnv(self) : \n",
    "        return self.env\n",
    "\n",
    "    def setPi(self,pi):\n",
    "        self.pi = pi\n",
    "\n",
    "\n",
    "vitesse_min = -11\n",
    "vitesse_max = 11\n",
    "STEP = 50\n",
    "        \n",
    "def arg_state(value):\n",
    "    start = time.time()\n",
    "    value = value\n",
    "    step = STEP\n",
    "    min_distance = -1\n",
    "    max_distance = 2.5\n",
    "    tab = np.linspace(min_distance,max_distance,step)\n",
    "    bestIndex = 0\n",
    "    index = 0\n",
    "    while index < len(tab) and value > tab[index]:\n",
    "        bestIndex = index\n",
    "        index+=1\n",
    "    end = time.time()\n",
    "    print(\"time : \",(end-start))\n",
    "    return bestIndex\n",
    "\n",
    "\n",
    "class FlappyAgent:\n",
    "    \"\"\"\n",
    "        Agent following a policy pi : pi is a dictionary state -> action\n",
    "    \"\"\"\n",
    "    def __init__(self,env,pi):\n",
    "        self.env = env\n",
    "        self.pi = pi\n",
    "\n",
    "    def act(self,obs):\n",
    "        horizontal_value,vertical_value = obs[0],obs[1]\n",
    "        return self.pi[get_state2(horizontal_value,vertical_value,self.env._game.player_vel_y)]\n",
    "\n",
    "    def store(self,obs,action,new_obs,reward):\n",
    "        pass\n",
    "\n",
    "    def getPi(self) :\n",
    "        return self.pi\n",
    "\n",
    "    def getEnv(self) : \n",
    "        return self.env\n",
    "\n",
    "    def setPi(self,pi):\n",
    "        self.pi = pi\n",
    "\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm    \n",
    "\n",
    "def argmaxQlearning(actions , state, Q) :\n",
    "    if len(actions) < 1 :\n",
    "        return None\n",
    "    else :  \n",
    "        best_action = actions[0]\n",
    "        best_value = Q[(state,best_action)]\n",
    "        for action in actions :\n",
    "            value = Q[(state,action)]\n",
    "            if value > best_value :\n",
    "                best_value = value\n",
    "                best_action = action\n",
    "        return best_action\n",
    "\n",
    "def maxQlearning(actions , state, Q) :\n",
    "    if len(actions) < 1 :\n",
    "        return None\n",
    "    else :  \n",
    "        best_action = actions[0]\n",
    "        best_value = Q[(state,best_action)]\n",
    "        for action in actions :\n",
    "            value = Q[(state,action)]\n",
    "            if value > best_value :\n",
    "                best_value = value\n",
    "        return best_value\n",
    "\n",
    "def Qlearning(env, gamma = 0.9, eps=0.1, alpha=0.05,nbiter = 1000000) :\n",
    "    step = STEP\n",
    "    \"\"\"1 - Initialisation of the Q-table \"\"\"\n",
    "    states = [(j,i,z) for i in range(-10,11,1) for j in range(0,305,5) for z in range(-10,11)]\n",
    "    actions = [0,1]\n",
    "    Q = dict()\n",
    "    actions = [0,1]\n",
    "    for state in states:\n",
    "        for action in actions :\n",
    "            Q[(state,action)] = 0\n",
    "    \n",
    "    \n",
    "    print(\"Execution Q learning algorithm\")\n",
    "    for _ in tqdm(range(nbiter)) :\n",
    "        env.reset()\n",
    "        \"\"\" On initialise le premier état \"\"\"\n",
    "        horizontal_value,vertical_value = env._get_observation()[0],env._get_observation()[1]\n",
    "        current_state = get_state2(horizontal_value,vertical_value,env._game.player_vel_y) # (arg_state(horizontal_value),arg_state(vertical_value),env._game.player_vel_y)\n",
    "        final_state_reached = False\n",
    "        \"\"\"2 - Tant qu'on a pas atteint un état final\"\"\"\n",
    "        iter = 0\n",
    "        reward_t = 0\n",
    "        while not final_state_reached :\n",
    "            \"\"\"2.1 - On doit choisir une action a faire \"\"\"\n",
    "            if random.uniform(0,1) < eps :\n",
    "                \"\"\"Exploration : on selectionne une action au hasard\"\"\"\n",
    "                action = actions[random.randint(0, (len(actions)-1))]\n",
    "            else :\n",
    "                \"\"\"Exploitation : on selectionne l'action avec la valeur maximale\"\"\"\n",
    "                action = argmaxQlearning(actions,current_state,Q)\n",
    "            \"\"\"2.2 - Mise à jour de la table Q\"\"\"\n",
    "            obs,reward,done,_ = env.step(int(action))\n",
    "            horizontal_value,vertical_value = obs[0],obs[1]\n",
    "\n",
    "            nextState = get_state2(horizontal_value,vertical_value,env._game.player_vel_y)\n",
    "            \n",
    "            \"\"\"2.3 - Verification de l'etat final\"\"\"\n",
    "            final_state_reached = done\n",
    "            if final_state_reached :\n",
    "                reward = -1000\n",
    "            \n",
    "            \n",
    "            # print(\"vitesse : \",env._game.player_vel_y)\n",
    "            \n",
    "            \n",
    "            # print((horizontal_value,vertical_value))\n",
    "            # print('state : ',nextState,' reward : ',reward_t)\n",
    "            # print((current_state,action))\n",
    "            Q[(current_state,action)] = Q[(current_state,action)] + (alpha * (reward + ((gamma * maxQlearning(actions , nextState, Q)) - Q[(current_state,action)])))\n",
    "            \n",
    "            \"\"\"2.4 - Mise à jour de l'état courant\"\"\"  \n",
    "            current_state = nextState\n",
    "            iter+=1\n",
    "            if iter > 300 :\n",
    "                print(\"yes\")\n",
    "        \"\"\"3 - transformation de la table Q en politique\"\"\" \n",
    "        #print(\"etat final atteint en \",iter,\"itérations\")\n",
    "    print(Q)\n",
    "    PI = dict()\n",
    "    for state in states:\n",
    "        #print(state)\n",
    "        bestAction = actions[0]\n",
    "        bestValue = Q[(state,bestAction)]\n",
    "        for action in actions : \n",
    "            value = Q[(state,action)]\n",
    "            if value > bestValue :\n",
    "                bestValue = value\n",
    "                bestAction = action\n",
    "        PI[state] = bestAction\n",
    "    # print(PI)\n",
    "    # print(Q.values())\n",
    "    return PI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f88b807-9094-48f9-af4e-3192a76e4b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56372887-4e27-475e-b530-4ea87d5bc217",
   "metadata": {},
   "outputs": [],
   "source": [
    "liste = np.array([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d164a04-e397-4a6b-a0ed-bc164431dacb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liste.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8a8b388-487c-4f53-9680-97ac54189307",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = {1 : list() , 2 : list()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5abd85e0-943e-4f40-8841-4749cbff7059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: [], 2: []}\n"
     ]
    }
   ],
   "source": [
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb7e0af8-eb3c-4f25-84e8-489f6a3d3f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sadness\n",
      "unlimited\n",
      "my\n"
     ]
    }
   ],
   "source": [
    "x = [\"my\", \"unlimited\", \"sadness\"]\n",
    "for i in reversed(x):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47ce06c-f466-432e-95ae-1e8e5d5b8c91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
